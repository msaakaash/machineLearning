# -*- coding: utf-8 -*-
"""Loss.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17fHv-4QRXA1HPRijw04q8zuZBWTUUlcU
"""

import numpy as np

"""**Actual and Predicted values**"""

actual_values= np.array([15, 25, 35, 45, 55])
predicted_values= np.array([18, 22, 38, 42, 52])

number_of_samples = len(actual_values)

"""**L1 loss**"""

l1 = np.sum(abs(actual_values-predicted_values))
print("L1 loss:",l1)

"""**Mean Absolute Error(MAE)**"""

mae = l1/number_of_samples
print("Mean Absolute Error:",mae)

"""**L2 Loss**"""

l2 = np.sum((actual_values-predicted_values)**2)
print("L2 loss:",l2)

"""**Mean Squared Error (MSE)**"""

mse = l2/number_of_samples
print("Mean Square Error:",mse)



"""**Binary Cross Entropy**"""

ytrue = np.array([0, 1, 1, 0, 1])
ypred = np.array([0.1, 0.9, 0.8, 0.2, 0.7])

def binary_cross_entropy(ytrue,ypred):
  bce = -np.mean((ytrue*np.log(ypred))+((1-ytrue)*np.log(1-ypred)))
  return bce

bce_loss = binary_cross_entropy(ytrue,ypred)
print("Binary Cross Entropy Loss:",bce_loss)

